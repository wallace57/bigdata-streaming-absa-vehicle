"""
==================================================
üß† Vehicle Consumer (YOLOv8 + Spark Streaming)
--------------------------------------------------
- Nh·∫≠n frame t·ª´ Kafka topic `vehicle-stream`
- Gi·∫£i m√£ base64 ‚Üí YOLOv8 ph√°t hi·ªán ph∆∞∆°ng ti·ªán
- T√≠nh t·ªïng l∆∞·ª£ng xe theo camera + th·ªùi gian
- Ghi k·∫øt qu·∫£ v√†o PostgreSQL
==================================================
"""

import base64
import json
import cv2
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, current_timestamp, udf, from_unixtime
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType
)
from ultralytics import YOLO
from pyspark.sql.functions import to_timestamp
# =====================================================
# 1Ô∏è‚É£ Kh·ªüi t·∫°o Spark session
# =====================================================
spark = (
    SparkSession.builder
    .appName("VehicleStreamingConsumer")
    .config(
        "spark.jars.packages",
        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,"
        "org.postgresql:postgresql:42.7.1"
    )
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

print("[Consumer] üöÄ Spark Structured Streaming started...")

# =====================================================
# 2Ô∏è‚É£ Schema c·ªßa d·ªØ li·ªáu Kafka t·ª´ vehicle_producer.py
# =====================================================
schema = StructType([
    StructField("camera_id", StringType(), True),
    StructField("timestamp", DoubleType(), True),
    StructField("frame_data", StringType(), True)  # base64 image
])

# =====================================================
# 3Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
# =====================================================
df_kafka = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "kafka:9092")
    .option("subscribe", "vehicle-stream")
    .option("startingOffsets", "latest")
    .load()
)

# =====================================================
# 4Ô∏è‚É£ Parse JSON
# =====================================================
df_parsed = (
    df_kafka
    .selectExpr("CAST(value AS STRING) AS json_value")
    .select(from_json(col("json_value"), schema).alias("data"))
    .select("data.*")
)

# =====================================================
# 5Ô∏è‚É£ H√†m YOLO detect trong UDF
# =====================================================

yolo_model = YOLO("yolov8n.pt")

def detect_vehicles(frame_b64):
    """Gi·∫£i m√£ base64 v√† ƒë·∫øm s·ªë xe + hi·ªÉn th·ªã log ra console"""
    try:
        img_data = base64.b64decode(frame_b64)
        nparr = np.frombuffer(img_data, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if frame is None:
            print("[Consumer] ‚ö†Ô∏è Frame decode failed.")
            return 0

        results = yolo_model(frame, verbose=False)
        vehicle_list = []
        count = 0
        for box in results[0].boxes:
            cls_name = yolo_model.names[int(box.cls)]
            if cls_name in ["car", "bus", "truck", "motorbike"]:
                count += 1
                vehicle_list.append(cls_name)

        types_str = ",".join(vehicle_list)
        count = len(vehicle_list)
        print(f"[Kafka ‚Üí YOLO] üöó Detected {count} vehicles in frame.")
        return (count, types_str)
    except Exception as e:
        print(f"[UDF Error] {e}")
        return 0

detect_schema = StructType([
    StructField("count", IntegerType(), True),
    StructField("vehicle_types", StringType(), True)
])

detect_udf = udf(detect_vehicles, detect_schema)

# =====================================================
# 6Ô∏è‚É£ √Åp d·ª•ng UDF ƒë·∫øm s·ªë xe + chuy·ªÉn ƒë·ªïi timestamp
# =====================================================
df_detected = (
    df_parsed
    .withColumn("detection", detect_udf(col("frame_data")))
    .withColumn("count", col("detection.count"))
    .withColumn("vehicle_type", col("detection.vehicle_types"))
    .withColumn("processed_at", current_timestamp())
    .withColumn("frame_time", from_unixtime(col("timestamp")).cast("timestamp"))
    .select("camera_id", "count", "vehicle_type", "frame_time", "processed_at")
)

# =====================================================
# 7Ô∏è‚É£ Ghi k·∫øt qu·∫£ v√†o PostgreSQL
# =====================================================
postgres_url = "jdbc:postgresql://postgres:5432/airflow"
postgres_properties = {
    "user": "airflow",
    "password": "airflow",
    "driver": "org.postgresql.Driver"
}

def write_to_postgres(batch_df, batch_id):
    row_count = batch_df.count()
    print(f"\n==============================")
    print(f"[Batch {batch_id}] Writing {row_count} rows to PostgreSQL")
    print("==============================")

    (
        batch_df.write
        .jdbc(
            url=postgres_url,
            table="vehicle_counts",
            mode="append",
            properties=postgres_properties
        )
    )

query = (
    df_detected.writeStream
    .outputMode("update")
    .foreachBatch(write_to_postgres)
    .option("checkpointLocation", "/opt/airflow/checkpoints/vehicle_consumer_cp")
    .start()
)

print("[Consumer] üß† Waiting for streaming data...")
query.awaitTermination()
